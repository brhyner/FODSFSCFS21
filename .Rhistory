for (k in 12:12) {
# second loop loops through the columns of the date data frames (= months)
# initial query
running_url <- paste0('https://api.nytimes.com/svc/search/v2/articlesearch.json?',
'begin_date=', begindate[i,k],
'&end_date=', enddate[i,k],
'&facet_filter=true&fq=news_desk%3A(%22Politics%22)',
'&api-key=',NYT_KEY,
sep = '')
# get max pages from initial query
initial_Query <- fromJSON(running_url)
maxPages <- round((initial_Query$response$meta$hits[1] / 10) - 1)
# make sure maxPages is not negative
if(maxPages < 0) {
maxPages <- 0
}
# if there is only one page of results, I might run into problems with the rate limit, therefore
Sys.sleep(6)
# get results from page 0 to max by using a third for loop
for (j in 0:maxPages) {
# get results from page j
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
# progress
print(paste0("Retrieving page ", j, " from ", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k]))))
# save page searches
results[[paste("Page", j, "in", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k])))]] <- nytSearch
# pause for 6 seconds so the rate limit per minute is not violated
Sys.sleep(7)
}
# progress
print(paste0("Month ", k, " complete!"))
}
# progress
print(paste0("Year ", i, " complete!"))
}
View(nytSearch)
View(initial_query)
View(initial_Query)
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T)
View(nytSearch)
for (i in 4:7) {
# first loop loops through the rows of the date data frames (= year)
for (k in 1:12) {
# second loop loops through the columns of the date data frames (= months)
# initial query
running_url <- paste0('https://api.nytimes.com/svc/search/v2/articlesearch.json?',
'begin_date=', begindate[i,k],
'&end_date=', enddate[i,k],
'&facet_filter=true&fq=news_desk%3A(%22Politics%22)',
'&api-key=',NYT_KEY,
sep = '')
# get max pages from initial query
initial_Query <- fromJSON(running_url)
maxPages <- round((initial_Query$response$meta$hits[1] / 10) - 1)
# make sure maxPages is not negative
if(maxPages < 0) {
maxPages <- 0
}
# if there is only one page of results, I might run into problems with the rate limit, therefore
Sys.sleep(6)
# get results from page 0 to max by using a third for loop
for (j in 0:maxPages) {
# get results from page j
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
# progress
print(paste0("Retrieving page ", j, " from ", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k]))))
# save page searches
results[[paste("Page", j, "in", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k])))]] <- nytSearch
# pause for 6 seconds so the rate limit per minute is not violated
Sys.sleep(7)
}
# progress
print(paste0("Month ", k, " complete!"))
}
# progress
print(paste0("Year ", i, " complete!"))
}
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T)
View(nytSearch)
for (i in 4:4) {
# first loop loops through the rows of the date data frames (= year)
for (k in 4:12) {
# second loop loops through the columns of the date data frames (= months)
# initial query
running_url <- paste0('https://api.nytimes.com/svc/search/v2/articlesearch.json?',
'begin_date=', begindate[i,k],
'&end_date=', enddate[i,k],
'&facet_filter=true&fq=news_desk%3A(%22Politics%22)',
'&api-key=',NYT_KEY,
sep = '')
# get max pages from initial query
initial_Query <- fromJSON(running_url)
maxPages <- round((initial_Query$response$meta$hits[1] / 10) - 1)
# make sure maxPages is not negative
if(maxPages < 0) {
maxPages <- 0
}
# if there is only one page of results, I might run into problems with the rate limit, therefore
Sys.sleep(6)
# get results from page 0 to max by using a third for loop
for (j in 0:maxPages) {
# get results from page j
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
# progress
print(paste0("Retrieving page ", j, " from ", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k]))))
# save page searches
results[[paste("Page", j, "in", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k])))]] <- nytSearch
# pause for 6 seconds so the rate limit per minute is not violated
Sys.sleep(7)
}
# progress
print(paste0("Month ", k, " complete!"))
}
# progress
print(paste0("Year ", i, " complete!"))
}
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
for (i in 4:4) {
# first loop loops through the rows of the date data frames (= year)
for (k in 5:12) {
# second loop loops through the columns of the date data frames (= months)
# initial query
running_url <- paste0('https://api.nytimes.com/svc/search/v2/articlesearch.json?',
'begin_date=', begindate[i,k],
'&end_date=', enddate[i,k],
'&facet_filter=true&fq=news_desk%3A(%22Politics%22)',
'&api-key=',NYT_KEY,
sep = '')
# get max pages from initial query
initial_Query <- fromJSON(running_url)
maxPages <- round((initial_Query$response$meta$hits[1] / 10) - 1)
# make sure maxPages is not negative
if(maxPages < 0) {
maxPages <- 0
}
# if there is only one page of results, I might run into problems with the rate limit, therefore
Sys.sleep(6)
# get results from page 0 to max by using a third for loop
for (j in 0:maxPages) {
# get results from page j
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
# progress
print(paste0("Retrieving page ", j, " from ", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k]))))
# save page searches
results[[paste("Page", j, "in", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k])))]] <- nytSearch
# pause for 6 seconds so the rate limit per minute is not violated
Sys.sleep(7)
}
# progress
print(paste0("Month ", k, " complete!"))
}
# progress
print(paste0("Year ", i, " complete!"))
}
for (i in 5:7) {
# first loop loops through the rows of the date data frames (= year)
for (k in 1:12) {
# second loop loops through the columns of the date data frames (= months)
# initial query
running_url <- paste0('https://api.nytimes.com/svc/search/v2/articlesearch.json?',
'begin_date=', begindate[i,k],
'&end_date=', enddate[i,k],
'&facet_filter=true&fq=news_desk%3A(%22Politics%22)',
'&api-key=',NYT_KEY,
sep = '')
# get max pages from initial query
initial_Query <- fromJSON(running_url)
maxPages <- round((initial_Query$response$meta$hits[1] / 10) - 1)
# make sure maxPages is not negative
if(maxPages < 0) {
maxPages <- 0
}
# if there is only one page of results, I might run into problems with the rate limit, therefore
Sys.sleep(6)
# get results from page 0 to max by using a third for loop
for (j in 0:maxPages) {
# get results from page j
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
# progress
print(paste0("Retrieving page ", j, " from ", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k]))))
# save page searches
results[[paste("Page", j, "in", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k])))]] <- nytSearch
# pause for 6 seconds so the rate limit per minute is not violated
Sys.sleep(7)
}
# progress
print(paste0("Month ", k, " complete!"))
}
# progress
print(paste0("Year ", i, " complete!"))
}
paste0(running_url, "&page=", j)
initial_Query$response$meta$hits[1]
running_url
nytSearch
for (i in 7:7) {
# first loop loops through the rows of the date data frames (= year)
for (k in 11:11) {
# second loop loops through the columns of the date data frames (= months)
# initial query
running_url <- paste0('https://api.nytimes.com/svc/search/v2/articlesearch.json?',
'begin_date=', begindate[i,k],
'&end_date=', enddate[i,k],
'&facet_filter=true&fq=news_desk%3A(%22Politics%22)',
'&api-key=',NYT_KEY,
sep = '')
# get max pages from initial query
initial_Query <- fromJSON(running_url)
maxPages <- round((initial_Query$response$meta$hits[1] / 10) - 1)
# make sure maxPages is not negative
if(maxPages < 0) {
maxPages <- 0
}
# if there is only one page of results, I might run into problems with the rate limit, therefore
Sys.sleep(6)
# get results from page 0 to max by using a third for loop
for (j in 7:maxPages) {
# get results from page j
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
# progress
print(paste0("Retrieving page ", j, " from ", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k]))))
# save page searches
results[[paste("Page", j, "in", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k])))]] <- nytSearch
# pause for 6 seconds so the rate limit per minute is not violated
Sys.sleep(7)
}
# progress
print(paste0("Month ", k, " complete!"))
}
# progress
print(paste0("Year ", i, " complete!"))
}
for (i in 7:7) {
# first loop loops through the rows of the date data frames (= year)
for (k in 12:12) {
# second loop loops through the columns of the date data frames (= months)
# initial query
running_url <- paste0('https://api.nytimes.com/svc/search/v2/articlesearch.json?',
'begin_date=', begindate[i,k],
'&end_date=', enddate[i,k],
'&facet_filter=true&fq=news_desk%3A(%22Politics%22)',
'&api-key=',NYT_KEY,
sep = '')
# get max pages from initial query
initial_Query <- fromJSON(running_url)
maxPages <- round((initial_Query$response$meta$hits[1] / 10) - 1)
# make sure maxPages is not negative
if(maxPages < 0) {
maxPages <- 0
}
# if there is only one page of results, I might run into problems with the rate limit, therefore
Sys.sleep(6)
# get results from page 0 to max by using a third for loop
for (j in 0:maxPages) {
# get results from page j
nytSearch <- fromJSON(paste0(running_url, "&page=", j), flatten = T) %>%
# save data in a data frame
data.frame()
# progress
print(paste0("Retrieving page ", j, " from ", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k]))))
# save page searches
results[[paste("Page", j, "in", month(ymd(begindate[i,k]), label = T), year(ymd(begindate[i,k])))]] <- nytSearch
# pause for 6 seconds so the rate limit per minute is not violated
Sys.sleep(7)
}
# progress
print(paste0("Month ", k, " complete!"))
}
# progress
print(paste0("Year ", i, " complete!"))
}
save.image("C:/Users/Bobatz Rhyner/Google Drive/Master/2nd Semester/Foundations of Data Science for Social Scientists/Final Project/Foundations of Data Science for Social Scientist/Data_complete.RData")
politics_nyt <- bind_rows(results)
write_rds(politics_nyt, file = "trump_politics_nyt.Rda")
save.image("C:/Users/Bobatz Rhyner/Google Drive/Master/2nd Semester/Foundations of Data Science for Social Scientists/Final Project/Foundations of Data Science for Social Scientist/Backup_June19.RData")
source("Scripts/Init.R")
load("trump_politics_nyt.Rda")
nyt <- readRDS("trump_politics_nyt.Rda")
colnames(nyt)
View(nyt)
df_type <- nyt %>%
group_by(response.docs.type_of_material) %>%
summarise(freq = n())
View(df_type)
avg_wc <- nyt &>&
group_by(response.docs.type_of_material) %>%
summarise(freq = n(),
avg_words = mean(response.docs.word_count, na.rm = T))
View(nyt)
df_type <- nyt %>%
group_by(response.docs.type_of_material) %>%
summarise(freq = n())
avg_wc <- nyt %>%
group_by(response.docs.type_of_material) %>%
summarise(freq = n(),
avg_words = mean(response.docs.word_count, na.rm = T))
View(avg_wc)
avg_wc <- nyt %>%
filter(response.docs.word_count > 0) %>%
group_by(response.docs.type_of_material) %>%
summarise(freq = n(),
avg_words = mean(response.docs.word_count, na.rm = T))
avg_wc
colnames(nyt)
nyt[1,10]
as_date(nyt[1,10])
nyt$pub.date <- as_date(nyt$response.docs.pub_date)
colnames(nyt)
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
geom_area()
gg1
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
geom_area(stat = "bin")
gg1
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
# geom_area(stat = "bin")
geom_line()
gg1
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
# geom_area(stat = "bin")
geom_bar(stat = "bin")
gg1
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
# geom_area(stat = "bin")
geom_bar(stat = "bin", binwidth = 10)
gg1
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
# geom_area(stat = "bin")
geom_bar(stat = "bin", binwidth = 20)
gg1
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
# geom_area(stat = "bin")
geom_bar(stat = "bin", binwidth = 15)
gg1
gg1 <- ggplot(nyt,
aes(x = pub.date)) +
# geom_area(stat = "bin")
geom_bar(stat = "bin", binwidth = 20)
gg1
most_art <- nyt %>%
group_by(pub.date) %>%
summarise(n.art = n()) %>%
arrange(desc(n.art))
View(most_art)
most_art <- nyt %>%
group_by(pub.date) %>%
summarise(n.art = n()) %>%
arrange(desc(n.art)) %>%
head(n = 10)
View(most_art)
table(is.na(nyt$response.docs.headline.name))
table(is.na(nyt$response.docs.headline.main))
nyt$Trump <- ifelse(grepl("Trump", nyt$response.docs.headline.main), 1, 0)
table(nyt$Trump)
sample(nyt$response.docs.headline.main[nyt$Trump == 1], 10)
# relative freq of Trump mentionings
sum(nyt$Trump)/nrow(nyt) * 100
trump_time <- nyt %>%
group_by(pub.date) %>%
summarise(n.articles = n(),
trump.articles = sum(Trump),
freq = sum(Trump) / n() * 100)
View(trump_time)
gg2 <- ggplot(trump_time, aes(x = pub.date)) +
geom_bar(stat = "bin", binwidth = 15)
gg2
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date)) +
geom_bar(stat = "bin", binwidth = 15)
gg2
trump_time[trump_time$trump.articles > 0,]
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date, y = freq)) +
geom_bar(stat = "bin", binwidth = 15)
gg2
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date, y = freq)) +
geom_bar(stat = "identity", binwidth = 15)
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date, y = freq)) +
# geom_bar(stat = "bin", binwidth = 15)
geom_bar()
gg2
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date, y = freq)) +
# geom_bar(stat = "bin", binwidth = 15)
geom_col()
gg2
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date, y = freq)) +
# geom_bar(stat = "bin", binwidth = 15)
geom_density()
gg2
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date, y = freq)) +
# geom_bar(stat = "bin", binwidth = 15)
geom_line()
gg2
gg2 <- ggplot(trump_time, aes(x = pub.date), color = Trump) +
geom_bar(stat = "bin", binwidth = 15)
gg2
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date)) +
geom_bar(stat = "bin", binwidth = 15)
gg2
trump_time_2 <- nyt %>%
group_by(pub.date, Trump) %>%
summarise(n.articles = n())
View(trump_time_2)
gg2 <- ggplot(trump_time, aes(x = pub.date), color = Trump) +
geom_bar(stat = "bin", binwidth = 15)
gg2 <- ggplot(trump_time[trump_time$trump.articles > 0,], aes(x = pub.date)) +
geom_bar(stat = "bin", binwidth = 15)
gg3 <- ggplot(trump_time, aes(x = pub.date), color = Trump) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date), color = Trump) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date, color = Trump)) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date)) +
geom_bar(stat = "bin", binwidth = 15,
aes(fill = Trump))
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date)) +
geom_bar(binwidth = 15,
aes(fill = Trump))
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date)) +
geom_bar()
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date)) +
geom_bar(aes(fill = Trump))
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date)) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date), fill = Trump) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date), color = Trump) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date, y = ..count.., fill = Trump)) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date, y = n.articles, fill = Trump)) +
geom_bar(stat = "bin", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date, y = n.articles, fill = Trump)) +
geom_bar(binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date, y = n.articles, fill = Trump)) +
geom_bar(stat = "identity", binwidth = 15)
gg3
gg3 <- ggplot(trump_time_2, aes(x = pub.date, y = n.articles, fill = factor(Trump))) +
geom_bar(stat = "identity")
gg3
trump_time_2 <- nyt %>%
mutate(mm.yyyy = format(as.Date(pub.date, "%Y-%m")))
View(trump_time_2)
trump_time_2 <- nyt %>%
mutate(mm.yyyy = format_ISO8601(pub.date, precision = "ym")))
trump_time_2 <- nyt %>%
mutate(mm.yyyy = format_ISO8601(pub.date, precision = "ym"))
View(trump_time_2)
trump_time_2 <- nyt %>%
mutate(yyyy.mm = format_ISO8601(pub.date, precision = "ym")) %>%
group_by(yyyy.mm, Trump)
trump_time_2
View(trump_time_2)
trump_time_2 <- nyt %>%
mutate(yyyy.mm = format_ISO8601(pub.date, precision = "ym")) %>%
group_by(yyyy.mm, Trump) %>%
summarise(n.articles = n())
trump_time_2
gg3 <- ggplot(trump_time_2, aes(x = yyyy.mm, y = n.articles, fill = factor(Trump))) +
geom_bar(stat = "identity")
gg3
gg3 <- ggplot(trump_time_2, aes(x = yyyy.mm, y = n.articles, fill = factor(Trump))) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 45))
gg3
gg3 <- ggplot(trump_time_2, aes(x = yyyy.mm, y = n.articles, fill = factor(Trump))) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 45)) +
scale_x_date(date_breaks = "2 months")
gg3
gg3 <- ggplot(trump_time_2, aes(x = yyyy.mm, y = n.articles, fill = factor(Trump))) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 45)) +
scale_x_date(breaks = trump_time_2$yyyy.mm[seq(1, length(trump_time_2$yyyy.mm), by = 2)])
gg3
library(knitr)
